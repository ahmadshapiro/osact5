{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d63a67-0a58-48c7-97e4-fd146838c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349737f7-7a37-44b6-ad79-508204797f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from utils import getData\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b14a36d-af54-405a-a5a9-746087557a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        \n",
    "        self.labels = data.label.values\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in data.text.values]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cb83d6-3868-4896-9508-859007cea1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout, model_name):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        output =  self.softmax(linear_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a9b3b56-c61a-42a6-977b-f39e34896bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "f1_fun = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9bd09ff-e3e9-4f95-b956-e5906ff1375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train, val, learning_rate, epochs, batch_size):\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_f1_train = 0 \n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "                \n",
    "                f1=f1_fun.compute(predictions=output.argmax(dim=1), references=train_label)['f1']\n",
    "                total_f1_train+=f1 \n",
    "                \n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            total_f1_val = 0 \n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "                    \n",
    "                    f1=f1_fun.compute(predictions=output.argmax(dim=1), references=val_label)['f1']\n",
    "                    total_f1_val+=f1 \n",
    "            \n",
    "            print(\n",
    "                f\"\"\"Epochs: {epoch_num + 1} | \\\n",
    "                Train Loss: {total_loss_train / len(train): .3f} | \\\n",
    "                Train Accuracy: {total_acc_train / len(train): .3f} | \\\n",
    "                Train F1: {total_f1_train / len(train): .3f} |\\\n",
    "                Val Loss: {total_loss_val / len(val): .3f} | \\\n",
    "                Val Accuracy: {total_acc_val / len(val): .3f} |\n",
    "                Val F1: {total_f1_val / len(val): .3f}\n",
    "                \"\"\")\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bbe794f-480e-479d-8f2f-1c9664daf5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = getData(sub_task=\"A\", return_type=\"pandas\")\n",
    "train_dataset = Dataset(data=train_df, tokenizer= BertTokenizer.from_pretrained(\"UBC-NLP/MARBERT\"))\n",
    "test_dataset = Dataset(data=test_df, tokenizer= BertTokenizer.from_pretrained(\"UBC-NLP/MARBERT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff593f9-ff08-4e1d-885f-e2d47f466668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "torch.manual_seed(2903)\n",
    "model = BertClassifier(dropout= 0.1, model_name=\"UBC-NLP/MARBERT\")\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d6c28f-007e-4f06-bbad-cfd333ba9864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87bbe10d5d749d3ac113a4b2167ff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 |                 Train Loss:  0.155 |                 Train Accuracy:  0.683 |                 Train F1:  0.045 |                Val Loss:  0.154 |                 Val Accuracy:  0.651 |\n",
      "                Val F1:  0.096\n",
      "                \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3458e2819bf49b8bb1dbc1de93490f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, train_dataset, test_dataset, LR, EPOCHS, batch_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
