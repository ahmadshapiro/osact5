{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f2dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c5cef3",
   "metadata": {},
   "source": [
    "# 1. Marbert v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06e8b4",
   "metadata": {},
   "source": [
    "## Dropout = 0, no preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be90a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model_name' : \"MarBertV1\",\n",
    "          'model_link' : \"UBC-NLP/MARBERT\",\n",
    "          'pre_proccessed' : False,\n",
    "          'patience' : 10,\n",
    "          'seed' : 2903, \n",
    "          'task' : \"A\",\n",
    "          'epochs' : 100, \n",
    "          'dropout_ratio' : 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfe8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c7c896b99937079e\n",
      "Reusing dataset csv (/home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-c7c896b99937079e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6502ce7be884d89bd4804a5b10bf2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-c7c896b99937079e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-0748a3908997b9fc.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e424a0341e34baa8fc7a18ea066541e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = run_baseline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776f6c6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8887\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 44440\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17776' max='44440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17776/44440 19:33 < 29:20, 15.14 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.575200</td>\n",
       "      <td>0.626323</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.584395</td>\n",
       "      <td>0.671737</td>\n",
       "      <td>0.729689</td>\n",
       "      <td>0.702507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.500600</td>\n",
       "      <td>0.579620</td>\n",
       "      <td>0.782906</td>\n",
       "      <td>0.767670</td>\n",
       "      <td>0.810644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.523100</td>\n",
       "      <td>0.513719</td>\n",
       "      <td>0.752597</td>\n",
       "      <td>0.731444</td>\n",
       "      <td>0.814705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.573441</td>\n",
       "      <td>0.778488</td>\n",
       "      <td>0.780718</td>\n",
       "      <td>0.776451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>0.654893</td>\n",
       "      <td>0.763124</td>\n",
       "      <td>0.748860</td>\n",
       "      <td>0.790045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.662768</td>\n",
       "      <td>0.760778</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>0.788398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.610006</td>\n",
       "      <td>0.780783</td>\n",
       "      <td>0.768084</td>\n",
       "      <td>0.801771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-2222\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-2222/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-2222/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-2222/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-2222/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-4444\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-4444/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-4444/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-4444/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-4444/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-6666\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-6666/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-6666/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-6666/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-6666/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-8888\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-8888/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-8888/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-8888/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-8888/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-11110\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-11110/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-11110/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-11110/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-11110/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-13332\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-13332/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-13332/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-13332/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-13332/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-15554\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-15554/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-15554/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-15554/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-15554/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.1/checkpoint-17776\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-17776/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-17776/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-17776/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.1/checkpoint-17776/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Checkpoints/MarBertV1_d_0.1/checkpoint-6666 (score: 0.7829059829059829).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17776, training_loss=0.5341015617446144, metrics={'train_runtime': 1174.4904, 'train_samples_per_second': 151.334, 'train_steps_per_second': 37.838, 'total_flos': 915731305963920.0, 'train_loss': 0.5341015617446144, 'epoch': 8.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd965dbf",
   "metadata": {},
   "source": [
    "## Dropout = 0.5, no preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908efe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model_name' : \"MarBertV1\",\n",
    "          'model_link' : \"UBC-NLP/MARBERT\",\n",
    "          'pre_proccessed' : False,\n",
    "          'patience' : 10,\n",
    "          'seed' : 2903, \n",
    "          'task' : \"A\",\n",
    "          'epochs' : 100, \n",
    "          'dropout_ratio' : 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d788ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c7c896b99937079e\n",
      "Reusing dataset csv (/home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-c7c896b99937079e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574ad3af1cc045cbb351927bb4f8f0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-c7c896b99937079e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-0748a3908997b9fc.arrow\n",
      "Loading cached processed dataset at /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-c7c896b99937079e/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-91dd47fb2e0b2841.arrow\n",
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = run_baseline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeba9479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8887\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 222200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28886' max='222200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 28886/222200 31:57 < 3:33:51, 15.07 it/s, Epoch 13/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.799928</td>\n",
       "      <td>0.788381</td>\n",
       "      <td>0.817316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>0.897299</td>\n",
       "      <td>0.783775</td>\n",
       "      <td>0.792519</td>\n",
       "      <td>0.777629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.895132</td>\n",
       "      <td>0.801134</td>\n",
       "      <td>0.802332</td>\n",
       "      <td>0.799986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.197300</td>\n",
       "      <td>1.096019</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>0.777585</td>\n",
       "      <td>0.767670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.110800</td>\n",
       "      <td>1.363801</td>\n",
       "      <td>0.778744</td>\n",
       "      <td>0.777828</td>\n",
       "      <td>0.779695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.119600</td>\n",
       "      <td>1.329157</td>\n",
       "      <td>0.749018</td>\n",
       "      <td>0.778591</td>\n",
       "      <td>0.745331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>1.494216</td>\n",
       "      <td>0.754383</td>\n",
       "      <td>0.779826</td>\n",
       "      <td>0.748719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.411300</td>\n",
       "      <td>1.497467</td>\n",
       "      <td>0.716355</td>\n",
       "      <td>0.759461</td>\n",
       "      <td>0.725130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.681500</td>\n",
       "      <td>0.628695</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.675500</td>\n",
       "      <td>0.629740</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.656700</td>\n",
       "      <td>0.632654</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>0.625803</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.405431</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.340945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-2222\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-2222/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-2222/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-2222/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-2222/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-4444\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-4444/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-4444/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-4444/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-4444/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-6666\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-6666/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-6666/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-6666/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-6666/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-8888\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-8888/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-8888/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-8888/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-8888/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-11110\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-11110/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-11110/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-11110/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-11110/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-13332\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-13332/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-13332/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-13332/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-13332/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-15554\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-15554/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-15554/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-15554/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-15554/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-17776\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-17776/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-17776/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-17776/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-17776/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-19998\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-19998/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-19998/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-19998/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-19998/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-22220\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-22220/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-22220/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-22220/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-22220/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-24442\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-24442/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-24442/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-24442/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-24442/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-26664\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-26664/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-26664/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-26664/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-26664/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Checkpoints/MarBertV1_d_0.5/checkpoint-28886\n",
      "Configuration saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-28886/config.json\n",
      "Model weights saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-28886/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-28886/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/MarBertV1_d_0.5/checkpoint-28886/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Checkpoints/MarBertV1_d_0.5/checkpoint-6666 (score: 0.8011336945288278).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28886, training_loss=0.399649470893909, metrics={'train_runtime': 1917.7715, 'train_samples_per_second': 463.402, 'train_steps_per_second': 115.864, 'total_flos': 1488217281880980.0, 'train_loss': 0.399649470893909, 'epoch': 13.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116c381",
   "metadata": {},
   "source": [
    "## Dropout = 0.5, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbe93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model_name' : \"MarBertV1\",\n",
    "          'model_link' : \"UBC-NLP/MARBERT\",\n",
    "          'pre_proccessed' : True,\n",
    "          'patience' : 10,\n",
    "          'seed' : 2903, \n",
    "          'task' : \"A\",\n",
    "          'epochs' : 100, \n",
    "          'dropout_ratio' : 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db6aa56c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-87434f9d42f04810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-87434f9d42f04810/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfbedfd1fcc4bed8ebad9816e4e7c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131c897437ce46f4a14cf4aa2f4212cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-87434f9d42f04810/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834a0abce1264f1d8f3175654c51f78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c5e3f31acd4e2280c960fdb27936d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47342a2c3d84570b1794f6eae18b71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = run_baseline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f8fc1ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8887\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 222200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24442' max='222200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 24442/222200 26:49 < 3:37:06, 15.18 it/s, Epoch 11/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602900</td>\n",
       "      <td>0.519934</td>\n",
       "      <td>0.809233</td>\n",
       "      <td>0.790356</td>\n",
       "      <td>0.845146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.391200</td>\n",
       "      <td>0.944744</td>\n",
       "      <td>0.779704</td>\n",
       "      <td>0.807051</td>\n",
       "      <td>0.772917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.905259</td>\n",
       "      <td>0.803150</td>\n",
       "      <td>0.794240</td>\n",
       "      <td>0.815299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.125800</td>\n",
       "      <td>1.082418</td>\n",
       "      <td>0.791891</td>\n",
       "      <td>0.798210</td>\n",
       "      <td>0.786926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>1.988078</td>\n",
       "      <td>0.704822</td>\n",
       "      <td>0.765163</td>\n",
       "      <td>0.733069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>1.489020</td>\n",
       "      <td>0.750016</td>\n",
       "      <td>0.781892</td>\n",
       "      <td>0.747342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>1.445190</td>\n",
       "      <td>0.773257</td>\n",
       "      <td>0.770236</td>\n",
       "      <td>0.776676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>1.295711</td>\n",
       "      <td>0.772479</td>\n",
       "      <td>0.785262</td>\n",
       "      <td>0.765201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>1.447083</td>\n",
       "      <td>0.759116</td>\n",
       "      <td>0.760670</td>\n",
       "      <td>0.757667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>1.682214</td>\n",
       "      <td>0.735301</td>\n",
       "      <td>0.768281</td>\n",
       "      <td>0.734603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>1.386109</td>\n",
       "      <td>0.753916</td>\n",
       "      <td>0.770828</td>\n",
       "      <td>0.746814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-2222\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-2222/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-2222/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-2222/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-2222/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-4444\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-4444/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-4444/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-4444/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-4444/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-6666\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-6666/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-6666/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-6666/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-6666/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-8888\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-8888/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-8888/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-8888/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-8888/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-11110\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-11110/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-11110/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-11110/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-11110/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-13332\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-13332/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-13332/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-13332/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-13332/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-15554\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-15554/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-15554/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-15554/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-15554/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-17776\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-17776/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-17776/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-17776/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-17776/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-19998\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-19998/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-19998/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-19998/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-19998/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-22220\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-22220/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-22220/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-22220/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-22220/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-24442\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-24442/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-24442/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-24442/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-24442/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Checkpoints/Pre_processed/MarBertV1_d_0.5/checkpoint-2222 (score: 0.8092327791467895).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24442, training_loss=0.17915107306282274, metrics={'train_runtime': 1610.4579, 'train_samples_per_second': 551.831, 'train_steps_per_second': 137.973, 'total_flos': 945643744070160.0, 'train_loss': 0.17915107306282274, 'epoch': 11.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119d58a",
   "metadata": {},
   "source": [
    "## Dropout = 0.1, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371883c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-87434f9d42f04810\n",
      "Reusing dataset csv (/home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-87434f9d42f04810/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cbf385ff364c858e210386ccb70911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-87434f9d42f04810/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-00551a39af4b513e.arrow\n",
      "Loading cached processed dataset at /home/ahmad.shapiro/.cache/huggingface/datasets/csv/default-87434f9d42f04810/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-6e4b930e3d52c461.arrow\n",
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = {'model_name' : \"MarBertV1\",\n",
    "          'model_link' : \"UBC-NLP/MARBERT\",\n",
    "          'pre_proccessed' : True,\n",
    "          'patience' : 10,\n",
    "          'seed' : 2903, \n",
    "          'task' : \"A\",\n",
    "          'epochs' : 100, \n",
    "          'dropout_ratio' : 0.1}\n",
    "trainer = run_baseline(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9916fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/ahmad.shapiro/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8887\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 222200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19617' max='222200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 19617/222200 21:04 < 3:37:35, 15.52 it/s, Epoch 8.83/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582700</td>\n",
       "      <td>0.489794</td>\n",
       "      <td>0.797076</td>\n",
       "      <td>0.784585</td>\n",
       "      <td>0.816665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.753711</td>\n",
       "      <td>0.775794</td>\n",
       "      <td>0.814482</td>\n",
       "      <td>0.774433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0.989969</td>\n",
       "      <td>0.802555</td>\n",
       "      <td>0.792920</td>\n",
       "      <td>0.816045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.110900</td>\n",
       "      <td>1.118458</td>\n",
       "      <td>0.774159</td>\n",
       "      <td>0.781464</td>\n",
       "      <td>0.768790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>1.026134</td>\n",
       "      <td>0.794673</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.793731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>1.384970</td>\n",
       "      <td>0.728066</td>\n",
       "      <td>0.706938</td>\n",
       "      <td>0.813420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.089200</td>\n",
       "      <td>1.381105</td>\n",
       "      <td>0.771871</td>\n",
       "      <td>0.789555</td>\n",
       "      <td>0.764073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>1.362350</td>\n",
       "      <td>0.759202</td>\n",
       "      <td>0.788078</td>\n",
       "      <td>0.754407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-2222\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-2222/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-2222/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-2222/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-2222/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-4444\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-4444/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-4444/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-4444/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-4444/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-6666\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-6666/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-6666/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-6666/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-6666/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-8888\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-8888/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-8888/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-8888/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-8888/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-11110\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-11110/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-11110/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-11110/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-11110/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-13332\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-13332/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-13332/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-13332/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-13332/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-15554\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-15554/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-15554/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-15554/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-15554/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1270\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-17776\n",
      "Configuration saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-17776/config.json\n",
      "Model weights saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-17776/pytorch_model.bin\n",
      "tokenizer config file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-17776/tokenizer_config.json\n",
      "Special tokens file saved in ./Checkpoints/Pre_processed/MarBertV1_d_0.1/checkpoint-17776/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6833c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT-CpuJupyter",
   "language": "python",
   "name": "mt-cpujupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
